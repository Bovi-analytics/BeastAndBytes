---
title: "Linear regression with a single predictor"
subtitle: "Lecture 4"
date: "February 10, 2025"
format: 
  revealjs:
    footer: "[https://bovi-analytics.github.io/BeastsAndBytes/](https://bovi-analytics.github.io/BeastsAndBytes/)"
---

# Warm up

```{r}
#| echo: false
#| message: false

library(countdown)
library(tidyverse)
ggplot2::theme_set(theme_gray(base_size = 16))
```

## Goals

-   Recap simple linear regression (Interpreting slopes and intercepts)
-   Linear regression with multiple predictors
-   Logistic regression

# Linear regression with a single predictor

## Simple linear regression {.smaller}

Use **simple linear regression** to model the relationship between a quantitative outcome ($Y$) and a single quantitative predictor ($X$): $$\Large{Y = \beta_0 + \beta_1 X + \epsilon}$$

::: incremental
-   $\beta_1$: True slope of the relationship between $X$ and $Y$
-   $\beta_0$: True intercept of the relationship between $X$ and $Y$
-   $\epsilon$: Error (residual)
:::

# Logistic regression

## What is logistic regression?

::::: columns
::: {.column width="50%"}
-   Similar to linear regression.... but

-   Modeling tool when our response is categorical
:::

::: {.column width="50%"}
![](images/logistic.png){fig-align="center"}
:::
:::::

## Modelling binary outcomes

-   Variables with binary outcomes follow the **Bernouilli distribution**:

    -   $y_i \sim Bern(p)$

    -   $p$: Probability of success

    -   $1-p$: Probability of failure

-   We can't model $y$ directly, so instead we model $p$

## Linear model

$$
p_i = \beta_o + \beta_1 \times X_1 + \cdots + \epsilon
$$

-   But remember that $p$ must be between 0 and 1

-   We need a **link function** that transforms the linear model to have an appropriate range

## Logit link function

The **logit** function take values between 0 and 1 (probabilities) and maps them to values in the range negative infinity to positive infinity:

$$
logit(p) = log \bigg( \frac{p}{1 - p} \bigg)
$$

```{r}
#| include: false

library(tidyverse)

tibble(
  x = seq(0.001, 0.999, 0.001),
  y = log(x / (1-x))
) |>
  ggplot(aes(x = x, y = y)) +
  geom_smooth() +
  scale_x_continuous(limits = c(0,1), breaks = c(0, 0.25, 0.5, 0.75, 1)) +
  labs(x = "p", y = "logit(p)", title = "logit(p) vs. p")
```

## This isn't exactly what we need though.....

-   Recall, the goal is to take values between -$\infty$ and $\infty$ and map them to probabilities.

-   We need the opposite of the link function... or the *inverse*

-   Taking the inverse of the logit function will map arbitrary real values back to the range \[0, 1\]

## Generalized linear model

-   We model the logit (log-odds) of $p$ :

$$
logit(p) = log \bigg( \frac{p}{1 - p} \bigg) = \beta_o + \beta_1 \times X1_i + \cdots + \epsilon 
$$

-   Then take the inverse to obtain the predicted $p$:

$$
p_i = \frac{e^{\beta_o + \beta_1 \times X1_i + \cdots + \epsilon}}{1 + e^{\beta_o + \beta_1 \times X1_i + \cdots + \epsilon}}
$$

## A logistic model visualized

```{r}
#| echo: false

sigmoid = function(x) 1 / (1 + exp(-x + 10))
plot.function(sigmoid, from = 0, to = 20, n = 101, 
              ylab="P(Y = 1)", 
              xlab = "X (predictor)", 
              main = "Predicted probability Y = 1", 
              lwd = 3)
```

## Takeaways

-   Generalized linear models allow us to fit models to predict non-continuous outcomes

-   Predicting binary outcomes requires modeling the log-odds of success, where p = probability of success
