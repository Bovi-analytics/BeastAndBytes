---
title: "Decision Trees"
subtitle: "Lecture 12"
date: "April 17, 2025"
format: 
  revealjs:
    footer: "[https://bovi-analytics.github.io/BeastsAndBytes/](https://bovi-analytics.github.io/BeastsAndBytes/)"
---

# Introduction to Decision Trees {#sec-introduction}

## Prerequisites {#sec-prerequisites}

-   This lecture will focus on **Decision Trees** and introduce **Random Forests**, powerful techniques for classification and regression tasks.
-   We'll use the `rpart` package for decision trees and the `randomForest` package for random forests.

```{r}
#| label: setup
#| echo: true
#| message: false

# Install necessary packages if not already installed
if(!require(tidyverse)) install.packages("tidyverse")
if(!require(rpart)) install.packages("rpart")
if(!require(rpart.plot)) install.packages("rpart.plot") # Install rpart.plot
if(!require(caret)) install.packages("caret")
if(!require(randomForest)) install.packages("randomForest")
if(!require(DiagrammeR)) install.packages("DiagrammeR")

# Load the libraries
library(tidyverse)
library(rpart)
library(rpart.plot) # Load rpart.plot
library(caret)
library(randomForest)
library(DiagrammeR) # Load DiagrammeR
```

# What are Decision Trees? {#sec-what-are-decision-trees}

## Decision Trees: An Intuitive Approach {#sec-dt-intuitive}

Decision trees are a widely-used and intuitive machine learning technique used to solve prediction problems. They work through a series of yes-no questions to arrive at an outcome.

> *Some of the conceptual explanation here is inspired by Shaw Talebi's excellent work on decision trees. For a Python-oriented approach, see his [blog post](https://medium.com/data-science/decision-trees-introduction-intuition-dac9592f4b7f).*

## A Simple Example {#sec-dt-example}

Imagine deciding whether to drink tea or coffee based on:
- Time of day
- Hours of sleep from last night

![Simple decision tree example](slides/images/Lecture-12/1.png)

## Decision Tree Structure {#sec-dt-structure}

A decision tree consists of:

- **Root Node**: The initial splitting point (Is it after 4 PM?)
- **Internal/Splitting Nodes**: Further split data based on conditions (Hours of sleep > 6?)
- **Leaf/Terminal Nodes**: Final outcome where no further splits occur (Tea üçµ or Coffee ‚òï)
- **Edges**: Connect nodes and represent decision paths (Yes/No)

## Using Decision Trees with Data {#sec-dt-with-data}

In practice, we use tabular data where each row is evaluated through the tree:

![Tabular data example](slides/images/Lecture-12/2.png)

## Graphical View of a Decision Tree {#sec-dt-graphical}

Decision trees partition the feature space into regions:

![Graphical view of decision tree partitioning](slides/images/Lecture-12/3.png)

# How to Grow a Decision Tree {#sec-grow-dt}

## Training Decision Trees from Data {#sec-dt-training}

Decision trees are grown using an optimization process:

1. **Starting Point**: Begin with all data in a single node
2. **Greedy Search**: Find the "best" variable and splitting point
3. **Recursive Splitting**: Repeat the process on each resulting partition
4. **Stopping**: Continue until a stopping criterion is met

## Split Criteria {#sec-split-criteria}

Decision trees use various metrics to determine the best split:

- **Classification Trees**:
  - Gini impurity (default in many implementations)
  - Information gain / Entropy reduction
  
- **Regression Trees**:
  - Mean Squared Error (MSE)
  - Mean Absolute Error (MAE)

## Overfitting in Decision Trees {#sec-dt-overfitting}

A fully grown tree might perfectly classify training data but perform poorly on new data:

![Comparison of simple vs. complex decision boundaries](slides/images/Lecture-12/4.png)

## Controlling Tree Growth {#sec-control-growth}

To prevent overfitting, we can use:

1. **Hyperparameter Tuning**:
   - Maximum tree depth
   - Minimum samples per leaf
   - Minimum samples for split
   
2. **Pruning**:
   - Grow a full tree, then remove branches that don't improve performance
   - Cost-complexity pruning

# Decision Trees in R {#sec-dt-in-r}

## The `rpart` Package {#sec-rpart-pkg}

R has excellent support for decision trees through the `rpart` package:

```{r}
#| echo: true

# Load a sample dataset - iris
data(iris)
head(iris)
```

## Building a Simple Decision Tree {#sec-simple-dt}

```{r}
#| echo: true

# Build a decision tree model
iris_tree <- rpart(Species ~ ., 
                  data = iris,
                  method = "class")

# Print the model
print(iris_tree)
```

## Visualizing the Decision Tree {#sec-visualize-dt}

```{r}
#| echo: true
#| fig.width: 8
#| fig.height: 6

# Plot the decision tree
rpart.plot(iris_tree, 
           extra = 104,      # Show sample sizes and percentage
           box.palette = "GnBu", # Color scheme
           fallen.leaves = TRUE) # Align leaves
```

## Interpreting the Tree {#sec-interpret-dt}

- At each node, we see:
  - The predicted class
  - The probability distribution
  - The percentage of observations
  - The splitting criterion

- For the iris dataset:
  - The first split is on Petal.Length < 2.5
  - This perfectly separates setosa from the other species
  - The second split uses Petal.Width to separate versicolor and virginica

# Practical Example: Diabetes Prediction {#sec-practical-example}

## Diabetes Dataset {#sec-diabetes-data}

Let's use the Pima Indians Diabetes Dataset to predict diabetes risk:

```{r}
#| echo: true

# Load diabetes data
diabetes_data <- read.csv("https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv", header = FALSE)

# Set column names
colnames(diabetes_data) <- c("Pregnancies", "Glucose", "BloodPressure", "SkinThickness", 
                            "Insulin", "BMI", "DiabetesPedigreeFunction", "Age", "Outcome")

# Check the data
glimpse(diabetes_data)
```

## Understanding the Dataset {#sec-understand-data}

The dataset contains information about females of Pima Indian heritage with the following variables:

- **Pregnancies**: Number of pregnancies
- **Glucose**: Plasma glucose concentration (2 hours after oral glucose tolerance test)
- **BloodPressure**: Diastolic blood pressure (mm Hg)
- **SkinThickness**: Triceps skin fold thickness (mm)
- **Insulin**: 2-Hour serum insulin (mu U/ml)
- **BMI**: Body mass index (weight in kg/(height in m)^2)
- **DiabetesPedigreeFunction**: A function of diabetes family history
- **Age**: Age in years
- **Outcome**: Class variable (0 = no diabetes, 1 = diabetes)

## Preprocessing the Data {#sec-data-prep}

```{r}
#| echo: true

# Convert target to factor for classification
diabetes_data$Outcome <- as.factor(diabetes_data$Outcome)

# Recode target for interpretability
levels(diabetes_data$Outcome) <- c("Healthy", "Diabetic")

# Handle missing values (zeros are unlikely values for many of these measurements)
diabetes_data$Glucose[diabetes_data$Glucose == 0] <- NA
diabetes_data$BloodPressure[diabetes_data$BloodPressure == 0] <- NA
diabetes_data$SkinThickness[diabetes_data$SkinThickness == 0] <- NA
diabetes_data$Insulin[diabetes_data$Insulin == 0] <- NA
diabetes_data$BMI[diabetes_data$BMI == 0] <- NA

# Impute missing values with median
for(i in 1:5) {
  diabetes_data[is.na(diabetes_data[,i+1]), i+1] <- median(diabetes_data[,i+1], na.rm = TRUE)
}

# Check class balance
table(diabetes_data$Outcome)
```

## Data Preparation {#sec-data-split}

```{r}
#| echo: true

# Split into training and testing sets
set.seed(123)
train_index <- createDataPartition(diabetes_data$Outcome, p = 0.7, list = FALSE)
train_data <- diabetes_data[train_index, ]
test_data <- diabetes_data[-train_index, ]

# Check class balance
prop.table(table(train_data$Outcome))
```

## Training the Model {#sec-train-model}

```{r}
#| echo: true

# Train a decision tree model
diabetes_tree <- rpart(Outcome ~ ., 
                      data = train_data,
                      method = "class",
                      control = rpart.control(cp = 0.01)) # complexity parameter

# View the result
rpart.plot(diabetes_tree, 
           extra = 106,      
           box.palette = "RdBu",
           fallen.leaves = TRUE,
           main = "Diabetes Classification Decision Tree")
```

## Evaluating the Model {#sec-evaluate-model}

```{r}
#| echo: true

# Make predictions on test data
predictions <- predict(diabetes_tree, test_data, type = "class")

# Create confusion matrix
conf_matrix <- confusionMatrix(predictions, test_data$Outcome)
conf_matrix
```

## Feature Importance {#sec-feature-importance}

```{r}
#| echo: true
#| fig.width: 8
#| fig.height: 5

# Extract variable importance
var_importance <- diabetes_tree$variable.importance
var_importance_df <- data.frame(
  Variable = names(var_importance),
  Importance = var_importance
)

# Plot variable importance
ggplot(var_importance_df, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(x = "Variables", y = "Importance", 
       title = "Variable Importance in Diabetes Prediction") +
  theme_minimal()
```

## Hyperparameter Tuning {#sec-hyperparameter}

We can tune the complexity parameter (cp) to control tree growth:

```{r}
#| echo: true
#| fig.width: 7
#| fig.height: 5

# Plot cp values
plotcp(diabetes_tree)

# Find optimal cp value
optimal_cp <- diabetes_tree$cptable[which.min(diabetes_tree$cptable[,"xerror"]),"CP"]
cat("Optimal CP value:", optimal_cp, "\n")

# Train model with optimal cp
pruned_tree <- prune(diabetes_tree, cp = optimal_cp)

# Plot pruned tree
rpart.plot(pruned_tree,
           extra = 106,
           box.palette = "RdBu",
           fallen.leaves = TRUE,
           main = "Pruned Diabetes Classification Tree")
```

## Model Comparison {#sec-model-comparison}

```{r}
#| echo: true

# Evaluate pruned model
pruned_predictions <- predict(pruned_tree, test_data, type = "class")
pruned_conf_matrix <- confusionMatrix(pruned_predictions, test_data$Outcome)

# Compare metrics
metrics_comparison <- data.frame(
  Model = c("Full Tree", "Pruned Tree"),
  Accuracy = c(conf_matrix$overall["Accuracy"], 
               pruned_conf_matrix$overall["Accuracy"]),
  Sensitivity = c(conf_matrix$byClass["Sensitivity"], 
                  pruned_conf_matrix$byClass["Sensitivity"]),
  Specificity = c(conf_matrix$byClass["Specificity"], 
                  pruned_conf_matrix$byClass["Specificity"])
)

metrics_comparison
```

# Advantages & Limitations {#sec-advantages-limitations}

## Advantages of Decision Trees {#sec-advantages}

- **Interpretability**: Easy to understand and visualize
- **No scaling required**: Doesn't require feature normalization
- **Handles mixed data types**: Can process numerical and categorical variables
- **Captures non-linear relationships**: Can model complex decision boundaries
- **Feature importance**: Naturally reveals important variables

## Limitations of Decision Trees {#sec-limitations}

- **Overfitting**: Prone to capturing noise in training data
- **Instability**: Small changes in data can result in very different trees
- **Bias toward variables with many levels**: Can favor categorical variables with many categories
- **Suboptimal global decisions**: Greedy approach may not find globally optimal tree
- **Limited prediction smoothness**: Step-wise predictions rather than smooth functions

# Introduction to Random Forests {#sec-random-forests}

## Ensemble Learning {#sec-ensemble}

Random Forests overcome many limitations of single decision trees through:

- **Bootstrap Aggregation (Bagging)**: Training many trees on random subsets of data
- **Feature Randomization**: Considering only a subset of features at each split
- **Voting/Averaging**: Combining predictions from all trees

```{r}
#| echo: false
#| fig.width: 8
#| fig.height: 4

# Create a simple visualization of Random Forest concept
par(mfrow = c(1, 3))

# Tree 1
plot(1, type = "n", xlim = c(0, 10), ylim = c(0, 10), 
     xlab = "", ylab = "", main = "Tree 1")
points(c(3, 4, 5, 2, 3), c(7, 8, 7.5, 8, 9), col = "blue", pch = 16)
points(c(7, 8, 7, 8, 9), c(3, 2, 4, 3, 2), col = "red", pch = 16)
abline(h = 6, col = "darkgreen", lwd = 2)

# Tree 2
plot(1, type = "n", xlim = c(0, 10), ylim = c(0, 10), 
     xlab = "", ylab = "", main = "Tree 2")
points(c(3, 4, 5, 2, 3), c(7, 8, 7.5, 8, 9), col = "blue", pch = 16)
points(c(7, 8, 7, 8, 9), c(3, 2, 4, 3, 2), col = "red", pch = 16)
abline(v = 6, col = "darkblue", lwd = 2)

# Tree 3
plot(1, type = "n", xlim = c(0, 10), ylim = c(0, 10), 
     xlab = "", ylab = "", main = "Combined Forest")
points(c(3, 4, 5, 2, 3), c(7, 8, 7.5, 8, 9), col = "blue", pch = 16)
points(c(7, 8, 7, 8, 9), c(3, 2, 4, 3, 2), col = "red", pch = 16)
abline(h = 6, col = "darkgreen", lwd = 1, lty = 2)
abline(v = 6, col = "darkblue", lwd = 1, lty = 2)
lines(c(0, 6, 6, 10), c(6, 6, 0, 0), col = "purple", lwd = 2)
```
![Random Forest Intuitive Implementation](slides/images/Lecture-12/5.png)

## Random Forest Implementation {#sec-rf-implement}

```{r}
#| echo: true

# Train Random Forest model
set.seed(123)
diabetes_rf <- randomForest(Outcome ~ ., 
                           data = train_data,
                           ntree = 100,    # Number of trees
                           mtry = sqrt(ncol(train_data)-1)) # Number of variables tried at each split

# Print the model
print(diabetes_rf)

# Variable importance
varImpPlot(diabetes_rf, main = "Variable Importance in Random Forest")
```

## Random Forest Performance {#sec-rf-performance}

```{r}
#| echo: true

# Make predictions
rf_predictions <- predict(diabetes_rf, test_data)
rf_conf_matrix <- confusionMatrix(rf_predictions, test_data$Outcome)

# Add to comparison
metrics_comparison <- rbind(metrics_comparison,
                           data.frame(
                             Model = "Random Forest",
                             Accuracy = rf_conf_matrix$overall["Accuracy"],
                             Sensitivity = rf_conf_matrix$byClass["Sensitivity"],
                             Specificity = rf_conf_matrix$byClass["Specificity"]
                           ))

metrics_comparison
```

## Out-of-Bag Error Estimation {#sec-oob-error}

One unique advantage of Random Forests is out-of-bag (OOB) error estimation:

```{r}
#| echo: true
#| fig.width: 7
#| fig.height: 5

# Plot error vs number of trees
plot(diabetes_rf, main = "Random Forest Error Rates vs Number of Trees")
legend("topright", colnames(diabetes_rf$err.rate), col = 1:3, lty = 1:3)
```

# Advanced Topics {#sec-advanced-topics}

## Tree Ensembles Beyond Random Forests {#sec-tree-ensembles}

Other tree-based ensemble methods include:

- **Gradient Boosting**: Builds trees sequentially, each correcting errors of previous trees
  - XGBoost, LightGBM, CatBoost
- **AdaBoost**: Boosts performance by giving higher weight to misclassified instances
- **Extremely Randomized Trees**: Adds additional randomization in splitting

## Regression Trees {#sec-regression-trees}

Decision trees can also be used for regression problems:

```{r}
#| echo: true
#| fig.width: 7
#| fig.height: 5

# Create a simple regression tree example
data(Boston, package = "MASS")

# Train a regression tree
boston_tree <- rpart(medv ~ ., data = Boston)

# Plot the regression tree
rpart.plot(boston_tree, 
           main = "Boston Housing Price Regression Tree",
           box.palette = "BuPu")
```

# Conclusion {#sec-conclusion}

## Summary {#sec-summary}

- Decision trees provide an intuitive approach to classification and regression
- They work by recursively partitioning data based on feature values
- Controlling tree growth is crucial to prevent overfitting
- Random forests improve performance by combining many trees
- Both algorithms provide interpretable models with built-in feature importance

## Key Takeaways {#sec-takeaways}

1. Decision trees are excellent for initial modeling and understanding data
2. Hyperparameter tuning and pruning help prevent overfitting
3. Random forests generally outperform single decision trees
4. Consider the tradeoff between interpretability and performance
5. These techniques work well across many domains and data types

## References {#sec-references}

1. Talebi, S. (2023). "Decision Trees: Introduction & Intuition". [Medium](https://medium.com/data-science/decision-trees-introduction-intuition-dac9592f4b7f)
2. Breiman, L., Friedman, J., Stone, C.J., & Olshen, R.A. (1984). "Classification and Regression Trees"
3. Breiman, L. (2001). "Random Forests". Machine Learning, 45(1), 5-32
4. James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). "An Introduction to Statistical Learning"
5. Smith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C., & Johannes, R.S. (1988). "Using the ADAP learning algorithm to forecast the onset of diabetes mellitus"
6. Normalized Nerd. (2022). "Random Forest Algorithm Clearly Explained!". (https://www.youtube.com/watch?v=v6VJ2RO66Ag&ab_channel=NormalizedNerd)

# Thank You! {#sec-thank-you}

## Questions? {#sec-questions}

Feel free to reach out with any questions about decision trees or random forests!

```{r}
#| echo: false

# Display the packages and versions used
sessionInfo()
```