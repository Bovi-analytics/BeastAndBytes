---
title: "Logistic regression"
subtitle: "Lecture 5"
date: "February 10, 2025"
format: 
  revealjs:
    footer: "[https://bovi-analytics.github.io/BeastsAndBytes/](https://bovi-analytics.github.io/BeastsAndBytes/)"
---

# Warm up

```{r}
#| echo: false
#| message: false

library(countdown)
library(dplyr)
library(tidyverse)
ggplot2::theme_set(theme_gray(base_size = 16))
```

## Goals

-   Recap simple linear regression
-   Recap linear regression with multiple predictors
-   Logistic regression

# Linear regression

## Simple linear regression {.smaller}

Use **simple linear regression** to model the relationship between a quantitative outcome ($Y$) and a single quantitative predictor ($X$): $$\Large{Y = \beta_0 + \beta_1 X + \epsilon}$$

::: incremental
-   $\beta_1$: True slope of the relationship between $X$ and $Y$
-   $\beta_0$: True intercept of the relationship between $X$ and $Y$
-   $\epsilon$: Error (residual)
:::

## Multiple linear regression {.smaller}

Use **multiple linear regression** to model the relationship between a quantitative outcome ($Y$) and a multiple quantitative predictors and factors ($X$): $$\Large{Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon}$$

::: incremental
-   $\beta_1$: True slope of the relationship between $X_1$ and $Y$
-   $\beta_2$: True slope of the relationship between $X_2$ and $Y$
-   $\beta_0$: True intercept of the relationship between $X$ and $Y$
-   $\epsilon$: Error (residual)
:::

# Logistic regression

## What is logistic regression?

::::: columns
::: {.column width="50%"}
-   Similar to linear regression.... but

-   Modeling tool when our response is categorical
:::

::: {.column width="50%"}
![](images/logistic.png){fig-align="center"}
:::
:::::

## Modelling binary outcomes

-   Variables with binary outcomes follow the **Bernouilli distribution**:

    -   $y_i \sim Bern(p)$

    -   $p$: Probability of success

    -   $1-p$: Probability of failure

-   We can't model $y$ directly, so instead we model $p$

## Linear model

$$
p_i = \beta_o + \beta_1 \times X_1 + \cdots + \epsilon
$$

-   But remember that $p$ must be between 0 and 1

-   We need a **link function** that transforms the linear model to have an appropriate range

## Logit link function

The **logit** function take values between 0 and 1 (probabilities) and maps them to values in the range negative infinity to positive infinity:

$$
logit(p) = log \bigg( \frac{p}{1 - p} \bigg)
$$

```{r}
#| include: false

library(tidyverse)

tibble(
  x = seq(0.001, 0.999, 0.001),
  y = log(x / (1-x))
) |>
  ggplot(aes(x = x, y = y)) +
  geom_smooth() +
  scale_x_continuous(limits = c(0,1), breaks = c(0, 0.25, 0.5, 0.75, 1)) +
  labs(x = "p", y = "logit(p)", title = "logit(p) vs. p")
```

## This isn't exactly what we need though.....

-   Recall, the goal is to take values between -$\infty$ and $\infty$ and map them to probabilities.

-   We need the opposite of the link function... or the *inverse*

-   Taking the inverse of the logit function will map arbitrary real values back to the range \[0, 1\]

## Generalized linear model

-   We model the logit (log-odds) of $p$ :

$$
logit(p) = log \bigg( \frac{p}{1 - p} \bigg) = \beta_o + \beta_1 \times X1_i + \cdots + \epsilon 
$$

-   Then take the inverse to obtain the predicted $p$:

$$
p_i = \frac{e^{\beta_o + \beta_1 \times X1_i + \cdots + \epsilon}}{1 + e^{\beta_o + \beta_1 \times X1_i + \cdots + \epsilon}}
$$

## A logistic model visualized

```{r}
#| echo: false

sigmoid = function(x) 1 / (1 + exp(-x + 10))
plot.function(sigmoid, from = 0, to = 20, n = 101, 
              ylab="P(Y = 1)", 
              xlab = "X (predictor)", 
              main = "Predicted probability Y = 1", 
              lwd = 3)
```

## Takeaways

-   Generalized linear models allow us to fit models to predict non-continuous outcomes

-   Predicting binary outcomes requires modeling the log-odds of success, where p = probability of success

# Example

## Read the data

```{r data-read}
#| echo: true

df_raw_inseminations <- read.csv('df_inseminations.csv')
```

## Data prep

-   Drop any columns with missing values using the `drop_na()` function
-   Convert FALSE/NEGATIVE to 0/1
-   Relevel to make Parity 1 the default

```{r data-prep-1}
#| echo: true

df_inseminations <- df_raw_inseminations %>% 
  dplyr::mutate(
    Pregnant = factor(case_when(
      ResultedInPregnancy == TRUE ~ 1, 
      ResultedInPregnancy == FALSE ~ 0
      )),
    Parity=factor(Parity),
    Parity=fct_relevel(Parity,c("1","2",">2"))
    ) %>% 
  drop_na() %>%
  dplyr::arrange(Parity)
```

## Data overview

```{r data-overview}
#| echo: true

df_inseminations %>%
  select(InseminationNumber , Parity, Pregnant)
```

## Data visualization

```{r}
#| echo: false

ggplot(
  df_inseminations,
  aes(
    x = factor(Parity,  level = c('1', '2', '>2')),
    fill = ResultedInPregnancy)
  ) + 
  geom_bar(position = "fill")+ 
  labs(
    x = "Parity" , 
    y = "Proportion pregnant"
  )
```

## Logistic regression model

How to interpret

```{r}
#| echo: true
cows_fit <- glm(Pregnant ~ Parity, data = df_inseminations, family = "binomial")
summary(cows_fit)
```

## Logistic regression interpretation 1

```{r}
#| echo: true
exp(coef(cows_fit))
```

## Logistic regression interpretation 2 

```{r}
#| echo: true
exp(confint(cows_fit))
```
