---
title: "Data transformations"
subtitle: "Lecture 8"
date: "March 17, 2025"
format: 
  revealjs:
    footer: "[https://bovi-analytics.github.io/BeastsAndBytes/](https://bovi-analytics.github.io/BeastsAndBytes/)"
---

# Warm up {#sec-warm-up}

## Setup {#sec-setup}

```{r}
#| label: load-pkg
#| message: true

library(tidyverse)
library(nycflights13)
ggplot2::theme_set(theme_gray(base_size = 16))
```

# Introduction {#sec-introduction}

## Prerequisites {#sec-prerequisites}

-   Numeric vectors are the backbone of data science
-   But we still need the tidyverse because we'll use these base R functions inside of tidyverse functions like `mutate()` and `filter()`.

```{r}
#| label: setup
#| echo: true
#| message: false

library(tidyverse)
library(nycflights13)
```

## Making numbers {#sec-making-numbers}

In most cases, you'll get numbers already recorded in one of R's numeric types:

-    integer

-   double.

In some cases, however, you'll encounter them as strings, possibly because you've created them by pivoting from column headers or because something has gone wrong in your data import process.

## Parsing numbers {#sec-parsing-numbers}

readr provides two useful functions for parsing strings into numbers: `parse_double()` and `parse_number()`. Use `parse_double()` when you have numbers that have been written as strings:

```{r}
x <- c("1.2", "5.6", "1e3")
parse_double(x)
```

Use `parse_number()` when the string contains non-numeric text that you want to ignore. This is particularly useful for currency data and percentages:

```{r}
x <- c("$1,234", "USD 3,513", "59%")
parse_number(x)
```

# Counts {#sec-counts}

The `dplyr::count()` is great for quick exploration and checks during analysis:

```{r}
flights |> count(dest)
```

## Counts {#sec-counts-2}

If you want to see the most common values, add `sort = TRUE`:

```{r}
flights |> count(dest, sort = TRUE)
```

If you want to see all the values:

-   `|> View()`

-   `|> print(n = Inf)`.

## Counts alternative {#sec-counts-3}

Same computation "by hand" with `group_by()`, `summarize()` and `n()`.

```{r}
flights |> 
  group_by(dest) |> 
  summarize(
    n = n(),
    delay = mean(arr_delay, na.rm = TRUE)
  )
```

## Count distinct {#sec-counts-4}

There are a couple of variants of `n()` and `count()` that you might find useful:

-   `n_distinct(x)` counts the number of distinct (unique) values of one or more variables. For example, we could figure out which destinations are served by the most carriers:

    ```{r}
    flights |> 
      group_by(dest) |> 
      summarize(carriers = n_distinct(carrier)) |> 
      arrange(desc(carriers))
    ```

## Weighted count {#sec-counts-5}

-   A weighted count is a sum. For example you could "count" the number of miles each plane flew:

    ```{r}
    flights |> 
      group_by(tailnum) |> 
      summarize(miles = sum(distance))
    ```

## Weighted count {#sec-counts-6}

Weighted counts are a common problem so `count()` has a `wt` argument that does the same thing:

```{{r}}
#| results: false
flights |> count(tailnum, wt = distance)
```

## Counts missing {#sec-counts-7}

-   You can count missing values by combining `sum()` and `is.na()`. In the `flights` dataset this represents flights that are cancelled:

    ```{r}
    flights |> 
      group_by(dest) |> 
      summarize(n_cancelled = sum(is.na(dep_time))) 
    ```

# Numeric transformations {#sec-numeric-transformations}

Transformation functions work well with `mutate()` because their output is the same length as the input.

## Minimum and maximum {#sec-minimum-and-maximum-1}

The arithmetic functions work with pairs of variables (or columns). Two closely related functions are `pmin()` and `pmax()`, which when given two or more variables will return the smallest or largest value in each row:

```{r}
df <- tribble(
  ~x, ~y,
  1,  3,
  5,  2,
  7, NA,
)

df |> 
  mutate(
    min = pmin(x, y, na.rm = TRUE),
    max = pmax(x, y, na.rm = TRUE)
  )
```

## Minimum and maximum {#sec-minimum-and-maximum-2}

Note that these are different to the summary functions `min()` and `max()` which take multiple observations and return a single value. You can tell that you've used the wrong form when all the minimums and all the maximums have the same value:

```{r}
df |> 
  mutate(
    min = min(x, y, na.rm = TRUE),
    max = max(x, y, na.rm = TRUE)
  )
```

## Modular arithmetic {#sec-modular-arithmetic-1}

Modular arithmetic is the technical name for the type of math you did before you learned about decimal places, i.e. division that yields a whole number and a remainder. In R, `%/%` does integer division and `%%` computes the remainder:

```{r}
1:10 %/% 3
1:10 %% 3
```

## Modular arithmetic {#sec-modular-arithmetic-2}

Modular arithmetic is handy for the `flights` dataset, because we can use it to unpack the `sched_dep_time` variable into `hour` and `minute`:

```{r}
flights |> 
  mutate(
    hour = sched_dep_time %/% 100,
    minute = sched_dep_time %% 100,
    .keep = "used"
  )
```

## Modular arithmetic {#sec-modular-arithmetic-3}

We can combine that with the `mean(is.na(x))` trick from @sec-logical-summaries to see how the proportion of cancelled flights varies over the course of the day. The results are shown in @fig-prop-cancelled.

```{r}
#| label: fig-prop-cancelled
#| fig-cap: | 
#|   A line plot with scheduled departure hour on the x-axis, and proportion
#|   of cancelled flights on the y-axis. Cancellations seem to accumulate
#|   over the course of the day until 8pm, very late flights are much
#|   less likely to be cancelled.
#| fig-alt: |
#|   A line plot showing how proportion of cancelled flights changes over
#|   the course of the day. The proportion starts low at around 0.5% at
#|   5am, then steadily increases over the course of the day until peaking
#|   at 4% at 7pm. The proportion of cancelled flights then drops rapidly
#|   getting down to around 1% by midnight.
flights |> 
  group_by(hour = sched_dep_time %/% 100) |> 
  summarize(prop_cancelled = mean(is.na(dep_time)), n = n()) |> 
  filter(hour > 1) |> 
  ggplot(aes(x = hour, y = prop_cancelled)) +
  geom_line(color = "grey50") + 
  geom_point(aes(size = n))
```

# Logarithms {#sec-logarithms}

## Three logarithms {#sec-three-logarithms}

In R, you have a choice of three logarithms:

-   `log()` (the natural log, base e),

-   `log2()` (base 2),

-   `log10()` (base 10).

## Logarithm recommendation {#sec-logarithm-recommendation}

We recommend using `log2()` or `log10()`:

-    `log2()` is easy to interpret because a difference of 1 on the log scale corresponds to doubling on the original scale and a difference of -1 corresponds to halving;

-   `log10()` is easy to back-transform because (e.g.) 3 is 10\^3 = 1000.

The inverse of `log()` is `exp()`; to compute the inverse of `log2()` or `log10()` you'll need to use `2^` or `10^`.

## Rounding {#sec-rounding}

Use `round(x)` to round a number to the nearest integer:

```{r}
round(123.456)
```

## Rounding digits {#sec-rounding-digits}

You can control the precision of the rounding with the second argument, `digits`. `round(x, digits)` rounds to the nearest `10^-n` so `digits = 2` will round to the nearest 0.01. This definition is useful because it implies `round(x, -3)` will round to the nearest thousand, which indeed it does:

```{r}
round(123.456, 2)  # two digits
round(123.456, 1)  # one digit
round(123.456, -1) # round to nearest ten
round(123.456, -2) # round to nearest hundred
```

## Rounding digits - weirdness {#sec-rounding-digits---weirdness}

There's one weirdness with `round()` that seems surprising at first glance:

```{r}
round(c(1.5, 2.5))
```

`round()` uses what's known as "round half to even" or Banker's rounding: if a number is half way between two integers, it will be rounded to the **even** integer. This is a good strategy because it keeps the rounding unbiased: half of all 0.5s are rounded up, and half are rounded down.

## Rounding digits - floor/ceiling {#sec-rounding-digits---floorceiling}

`round()` is paired with `floor()` which always rounds down and `ceiling()` which always rounds up:

```{r}
x <- 123.456

floor(x)
ceiling(x)
```

## Cutting numbers into ranges {#sec-cutting-numbers-into-ranges-1}

Use `cut()`[^1] to break up (aka bin) a numeric vector into discrete buckets:

[^1]: ggplot2 provides some helpers for common cases in `cut_interval()`, `cut_number()`, and `cut_width()`. ggplot2 is an admittedly weird place for these functions to live, but they are useful as part of histogram computation and were written before any other parts of the tidyverse existed.

```{r}
x <- c(1, 2, 5, 10, 15, 20)
cut(x, breaks = c(0, 5, 10, 15, 20))
```

## Cutting numbers into ranges {#sec-cutting-numbers-into-ranges-2}

The breaks don't need to be evenly spaced:

```{r}
cut(x, breaks = c(0, 5, 10, 100))
```

## Cutting numbers into ranges {#sec-cutting-numbers-into-ranges-3}

You can optionally supply your own `labels`. Note that there should be one less `labels` than `breaks`.

```{r}
cut(x, 
  breaks = c(0, 5, 10, 15, 20), 
  labels = c("sm", "md", "lg", "xl")
)
```

## Cutting numbers into ranges {#sec-cutting-numbers-into-ranges-4}

Any values outside of the range of the breaks will become `NA`:

```{r}
y <- c(NA, -10, 5, 10, 30)
cut(y, breaks = c(0, 5, 10, 15, 20))
```

See the documentation for other useful arguments like `right` and `include.lowest`, which control if the intervals are `[a, b)` or `(a, b]` and if the lowest interval should be `[a, b]`.

## Cumulative and rolling aggregates {#sec-cumulative-and-rolling-aggregates}

Base R provides `cumsum()`, `cumprod()`, `cummin()`, `cummax()` for running, or cumulative, sums, products, mins and maxes. dplyr provides `cummean()` for cumulative means. Cumulative sums tend to come up the most in practice:

```{r}
x <- 1:10
cumsum(x)
```

If you need more complex rolling or sliding aggregates, try the [slider](https://slider.r-lib.org/) package.

# General transformations {#sec-general-transformations}

The following sections describe some general transformations which are often used with numeric vectors, but can be applied to all other column types.

## Ranks {#sec-ranks-1}

dplyr provides a number of ranking functions inspired by SQL, but you should always start with `dplyr::min_rank()`. It uses the typical method for dealing with ties, e.g., 1st, 2nd, 2nd, 4th.

```{r}
x <- c(1, 2, 2, 3, 4, NA)
min_rank(x)
```

## Ranks {#sec-ranks-2}

Note that the smallest values get the lowest ranks; use `desc(x)` to give the largest values the smallest ranks:

```{r}
min_rank(desc(x))
```

## Ranks alternatives {#sec-ranks-alternatives}

If `min_rank()` doesn't do what you need, look at the variants `dplyr::row_number()`, `dplyr::dense_rank()`, `dplyr::percent_rank()`, and `dplyr::cume_dist()`. See the documentation for details.

```{r}
df <- tibble(x = x)
df |> 
  mutate(
    row_number = row_number(x),
    dense_rank = dense_rank(x),
    percent_rank = percent_rank(x),
    cume_dist = cume_dist(x)
  )
```

## Offsets {#sec-offsets}

`dplyr::lead()` and `dplyr::lag()` allow you to refer to the values just before or just after the "current" value. They return a vector of the same length as the input, padded with `NA`s at the start or end:

```{r}
x <- c(2, 5, 11, 11, 19, 35)
lag(x)
lead(x)
```

## Offsets - lag {#sec-offsets---lag}

-   `x - lag(x)` gives you the difference between the current and previous value.

    ```{r}
    x - lag(x)
    ```

-   `x == lag(x)` tells you when the current value changes.

    ```{r}
    x == lag(x)
    ```

You can lead or lag by more than one position by using the second argument, `n`.

## Consecutive identifiers {#sec-consecutive-identifiers-1}

Sometimes you want to start a new group every time some event occurs. For example, when you're looking at website data, it's common to want to break up events into sessions, where you begin a new session after a gap of more than `x` minutes since the last activity. For example, imagine you have the times when someone visited a website:

```{r}
events <- tibble(
  time = c(0, 1, 2, 3, 5, 10, 12, 15, 17, 19, 20, 27, 28, 30)
)

```

## Consecutive identifiers {#sec-consecutive-identifiers-2}

And you've computed the time between each event, and figured out if there's a gap that's big enough to qualify:

```{r}
events <- events |> 
  mutate(
    diff = time - lag(time, default = first(time)),
    has_gap = diff >= 5
  )
events
```

## Consecutive identifiers {#sec-consecutive-identifiers-3}

But how do we go from that logical vector to something that we can `group_by()`? `cumsum()`, from @sec-cumulative-and-rolling-aggregates, comes to the rescue as gap, i.e. `has_gap` is `TRUE`, will increment `group` by one (@sec-numeric-summaries-of-logicals):

```{r}
events |> mutate(
  group = cumsum(has_gap)
)
```

## Consecutive identifiers {#sec-consecutive-identifiers-4}

Another approach for creating grouping variables is `consecutive_id()`, which starts a new group every time one of its arguments changes. For example, inspired by [this stackoverflow question](https://stackoverflow.com/questions/27482712), imagine you have a data frame with a bunch of repeated values:

```{r}
df <- tibble(
  x = c("a", "a", "a", "b", "c", "c", "d", "e", "a", "a", "b", "b"),
  y = c(1, 2, 3, 2, 4, 1, 3, 9, 4, 8, 10, 199)
)
```

## Consecutive identifiers {#sec-consecutive-identifiers-5}

If you want to keep the first row from each repeated `x`, you could use `group_by()`, `consecutive_id()`, and `slice_head()`:

```{r}
df |> 
  group_by(id = consecutive_id(x)) |> 
  slice_head(n = 1)
```

# Numeric summaries {#sec-numeric-summaries}

Just using the counts, means, and sums that we've introduced already can get you a long way, but R provides many other useful summary functions. Here is a selection that you might find useful.

## Center {#sec-center}

-   Function `mean()`
-   Function `median()`

## Minimum, maximum, and quantiles {#sec-minimum-maximum-and-quantiles-1}

-   `min()` and `max()` will give you the largest and smallest values.
-   Another powerful tool is `quantile()` which is a generalization of the median:
    -   `quantile(x, 0.25)` will find the value of `x` that is greater than 25% of the values,
    -   `quantile(x, 0.5)` is equivalent to the median,
    -   `quantile(x, 0.95)` will find the value that's greater than 95% of the values.

## Minimum, maximum, and quantiles {#sec-minimum-maximum-and-quantiles-2}

For the `flights` data, you might want to look at the 95% quantile of delays rather than the maximum, because it will ignore the 5% of most delayed flights which can be quite extreme.

```{r}
flights |>
  group_by(year, month, day) |>
  summarize(
    max = max(dep_delay, na.rm = TRUE),
    q95 = quantile(dep_delay, 0.95, na.rm = TRUE),
    .groups = "drop"
  )
```

## Variation {#sec-variation}

Two commonly used summaries are

-   the standard deviation `sd(x)`,

-   the inter-quartile range, `IQR()`.

## Positions {#sec-positions}

There's one final type of summary that's useful for numeric vectors, but also works with every other type of value: extracting a value at a specific position:

-   `first(x)`,

-   `last(x)`, and

-   `nth(x, n)`.

## Positions {#sec-positions-2}

For example, we can find the first, fifth and last departure for each day:

```{r}
flights |> 
  group_by(year, month, day) |> 
  summarize(
    first_dep = first(dep_time, na_rm = TRUE), 
    fifth_dep = nth(dep_time, 5, na_rm = TRUE),
    last_dep = last(dep_time, na_rm = TRUE)
  )
```

NB: Because dplyr functions use `_` to separate components of function and arguments names, these functions use `na_rm` instead of `na.rm`.

## With `mutate()` {#sec-with-mutate}

As the names suggest, the summary functions are typically paired with `summarize()`. However, because of the recycling rules we discussed in @sec-recycling they can also be usefully paired with `mutate()`, particularly when you want do some sort of group standardization. For example:

-   `x / sum(x)` calculates the proportion of a total.
-   `(x - mean(x)) / sd(x)` computes a Z-score (standardized to mean 0 and sd 1).
-   `(x - min(x)) / (max(x) - min(x))` standardizes to range \[0, 1\].
-   `x / first(x)` computes an index based on the first observation.
